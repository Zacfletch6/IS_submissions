{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtwtpvLUTGSAtRcXdY/wg9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zacfletch6/IS_submissions/blob/main/Final_project_DM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project\n",
        "\n",
        "By Zachary Fletcher\n",
        "12/1/2025"
      ],
      "metadata": {
        "id": "2nskDVRxYFK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "Import libraries and dataset"
      ],
      "metadata": {
        "id": "VnuqxGaIZl1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHNFqIdgXkto"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for each import:**\n",
        "* pandas - I imported pandas to read the csv, create and manipulate dataframes, and use descriptive methods like .describe(), .info(), .dtypes() and .isnull().\n",
        "* numpy - Numpy is highly compatible with many libaries like sklearn, matplotlib, and pandas. It is good for doing calculations on dataframes.\n",
        "* matplotlib - Matplotlib has various vizualizations which can be used during EDA and model evaluation.\n",
        "* warnings - Many libraries illicit warnings each time a relevant model is used. They get annoying and so I remove them.\n",
        "* DummyCleassifier - used a benchmark to compare the performance of other models.\n",
        "* train_test_split - used to split the data into training and testing sets for modeling.\n",
        "* sklearn.tree - This allows us to make the decision tree model using DecisionTreeClassifier and plot the tree with tree.tree_plot().\n",
        "* GridSearchCV - Allows me to test many different hyperparameters for each model.\n",
        "* sklearn.metrics - this will allow me to evaluate the performance of each model.\n",
        "* KNeighborsClassifier - This allows me to make the KNN classification model.\n",
        "* SVC - Allows me to make an SVC model.\n",
        "* MLPClassifier - allows me to make an MLP neural network.\n"
      ],
      "metadata": {
        "id": "yxSCY5gTYXIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/matthewpecsok/4482_fall_2024/main/data/census.csv'\n",
        "raw_df = pd.read_csv(url)\n",
        "df = raw_df.copy()"
      ],
      "metadata": {
        "id": "PPHS0HjScz96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a dataframe?**\n",
        "\n",
        "A dataframe is an object from the pandas library, like a 2 way table with rows and columns.\n"
      ],
      "metadata": {
        "id": "xBIo2LWgfH-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "High Level EDA"
      ],
      "metadata": {
        "id": "4e4_CtjZyqpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('---Head---')\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n---Dataframe Info---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n---Data Types---\")\n",
        "display(df.dtypes)\n",
        "\n",
        "print(\"\\n---Null Values---\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "print(\"\\n---Descriptive Statistics---\")\n",
        "display(df.describe())\n",
        "\n",
        "print('\\n---Dataframe Shape---')\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\n---Number of duplicate rows---\")\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "trhgKMD7zN0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of high level EDA:**\n",
        "\n",
        "There are 14 features within the dataset. There are no missing values within the column, so deletion and imputation should not be needed. There are 6 numerical features, 8 categorical features. A few columns may have outliers (hours-per-week, capital-gain, capital-loss). However, capital-gain and capital-loss also are very positively skewed. There are 24 duplicates."
      ],
      "metadata": {
        "id": "_ipk7V2v1jiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "Feature level EDA"
      ],
      "metadata": {
        "id": "jlaQWt318jMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts of each categorical column\n",
        "cat_cols = df.select_dtypes(include='object')\n",
        "\n",
        "for col in  cat_cols:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.countplot(x=col, data=df)\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Count')\n",
        "  plt.title(f'{col} Distribution')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "H7cGWuFyZSIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exact top 10 value counts of each column\n",
        "for col in  cat_cols:\n",
        "  print(f\"\\n---{col}---\")\n",
        "  display(df[col].value_counts().sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "FRkb4eOr07KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# statistics of numerical columns\n",
        "display(df.describe())"
      ],
      "metadata": {
        "id": "URx98J3E9yd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show distributions for every numerical row\n",
        "num_cols = df.select_dtypes(include='int64')\n",
        "\n",
        "for col in  num_cols:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.histplot(x=col, data=df)\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Count')\n",
        "  plt.title(f'{col} Distribution')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "k3k6BErNsbDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show boxplots for every numerical column\n",
        "num_cols = df.select_dtypes(include='int64')\n",
        "\n",
        "for col in num_cols.columns:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.boxplot(y=df[col])\n",
        "  plt.ylabel(col)\n",
        "  plt.title(f'Boxplot of {col}')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "T0YVBkhFv4cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretations of each column:**\n",
        "* **age** - Mean age is 38 yrs with deviation of +- 13 yrs. There are specific ages that seem to be very common, specifically those above a count of 900. It's skewed toward residents about 40 yrs and younger.\n",
        "* **work class** - The vast majority of people work in the private sector, specifically 22696 people. Barely anyone works without pay or has never worked.\n",
        "* **fnlwgt** - Mean of 1.90 with a deviation of +- 1 weight. The distribution has a few outliers. Most common weight is actually about 0.2, with quartile range between 0.1 and 0.3. This is likely a statistical weight, rather than a predictive feature, representing the 'weight' of each demographic group.\n",
        "* **education** - Top 3 categories include HS, some college, and bachelors, with a c ount of 10501, 7291, and 5355 respectively.\n",
        "* **education-num** - Mean of 10.08, and a standard deviation of +- 2.57. The Most common education numbers are 9, 10, and 13. There are a few outliers of 4 and below. Assuming this aligns with education, this should be highschool, some college, and bachelors, respectively.\n",
        "* **marital-status** - The 2 largest categories are married and never-married with a count of 14976 and 10683, respectively.\n",
        "* **occupation** - The top 5 occupational categories included Prof-specialty\tat 4140, Craft-repair\tat 4099, Exec-managerial at 4066, Adm-clerical at 3770, and Sales at 3650.\n",
        "* **relationship** - The top 3 relationships are Husband at 13193, Not-in-family at 8305, and Own-child at 5068. Notably, wives are the least common.\n",
        "* **race** - The vast majority of people are white, followed by black, with a count of 27816 and 3124, respectively.\n",
        "* **sex** - There are nearly twice as many males then females, with a count of 21790 males and 10771 females.\n",
        "* **capital-gain** - Mean is 1077.65 with a standard deviation of 7385.29. The median is 0, and the vast majority of people have captial gains lower than 2000. There are some extreme outliers.\n",
        "* **capital-loss** - Mean is 87.30 with a standard deviation of 402.96. The median is 0, with a decent amount of outliers ranging from about 500-3000.\n",
        "* **hours-per-week** - Mean is 40.43, a standard work week. Standard deviation is +- 12.34. The median is 40.0, with a minimum of 0 and a maximum of 99. This means there are outliers in below 40 and above 40.\n",
        "* **native-country** - The vast majority of people are native to the US.\n",
        "\n",
        "\n",
        "Demographic Pattern: Combining these columns reveals the most common demographic of those who participated in the census, those being white male American, married or single, about 40 years old, who work in the private sector 40 hours a week."
      ],
      "metadata": {
        "id": "C9lr28iU98jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "\n",
        "EDA of target variable only"
      ],
      "metadata": {
        "id": "xVlOvmI_pG4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='y', data=df)\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Target Variable (y)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UAmv0AYqpPrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df['y'].value_counts().sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "w2OA5Kb6cz__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy of majority classifier:\n",
        "majority_classifier_accuracy = round(df['y'].value_counts()['<=50K']/len(df), 2)\n",
        "print(f'Majority Classifier Accuracy: {majority_classifier_accuracy}')"
      ],
      "metadata": {
        "id": "LZyIyIexkkKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Is the data mining task a classification or regression task? In a text block explain the difference between the two.**\n",
        "\n",
        "Target variable 'y' is a categorical variable. Given that regression can only calculate numerical\n",
        "\n"
      ],
      "metadata": {
        "id": "mND_Bf2rqk2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Is the target variable balanced or imbalanced? Why is this important?**\n",
        "\n",
        "The target variable is imbalanced, with <=50k being greater than >50k. This is impotant because having the same number of instances means each class will have the same impact on the model. Because <=50k is the dominant class, it will disproportionately impact the model. if we predicted only based on the majority classifier, it would predict <=50k every time."
      ],
      "metadata": {
        "id": "8-l2hYptGfam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What a majority classifier's accuracy would be on this target variable. ie what accuracy do we need to beat to do better than a majority classifier?**\n",
        "\n",
        "The accuracy is 0.76 = 24720 / 32561."
      ],
      "metadata": {
        "id": "QrozBZLqGmnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What classification performance metrics will you be using based on what you have learned about the target variable. Rank the metrics you will be using from most important to least important and explain your reasoning for the ranking.**\n",
        "\n",
        "Given I will be using classification to predict a categorical variable, I will measure each model with accuracy, precision, recall, and F1 score. I would rank each metric like this: 1. Accuracy, 2. F1 score, 3. precision, 4. recall. My reasoning is that accuracy shows the overall performance of the model, and will be paramount to comparing which models performed the best overall. F1 is second because it is a combination of precision and recall, showing the model's ability to make correct predictions. recall and precision are last because they are further metrics of evaluating model performance. Recall measures the ability to correct classify instances belonging to a particular class. Precision measures the proportion of predicted instances actually in the predicted class."
      ],
      "metadata": {
        "id": "mRpXuxZQGqQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "\n",
        "EDA on Target variables with predictors"
      ],
      "metadata": {
        "id": "zRDq7Yl2cXMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, hue='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TAPtE_4l1Oox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f648eae1"
      },
      "source": [
        "# Correlation Matrix Heatmap of numerical variables\n",
        "num_cols = df.select_dtypes(include='int64')\n",
        "correlation_matrix = num_cols.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bargraph of average value for each numerical feature grouped by target variable\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).groupby(df['y']).mean()\n",
        "\n",
        "for col in num_cols:\n",
        "    num_cols[col].plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
        "    plt.title(f'Mean {col} by Target Variable')\n",
        "    plt.xlabel('Target Variable)')\n",
        "    plt.ylabel(f'Mean {col}')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XfAonoDTHFCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24cb79f1"
      },
      "source": [
        "#Boxplots of numerical variables by target variable 'y'\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "for col in numerical_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='y', y=col, data=df)\n",
        "    plt.title(f'Distribution of {col} by Target Variable (y)')\n",
        "    plt.xlabel('Target Variable (y)')\n",
        "    plt.ylabel(col)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e88a97f2"
      },
      "source": [
        "# graph of proportions of y in each category\n",
        "for col in cat_cols.columns:\n",
        "    crosstab_df = pd.crosstab(df[col], df['y'])\n",
        "    proportional_crosstab = crosstab_df.div(crosstab_df.sum(1).astype(float), axis=0)\n",
        "\n",
        "    proportional_crosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "    plt.title(f'Proportional Distribution of Target Variable (y) by {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.legend(title='y')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your interpretation of any predictors that appear to have an obvious relationship with the target. Which columns do you suspect will be most useful in predicting the target variable and why?**\n",
        "\n",
        "\n",
        "Those who earn >50k work more hours, have higher age overall, and generally have a higher education number. People who are older tend to hand higher capital gains. A higher proportion of married couples earn >50k. Occupations like exec-managerial and prof-specialty have higher proportions of >50k earnings. I suspect there is a relationship the features age, hours worked, occuptation, education, and marital status and the target variable."
      ],
      "metadata": {
        "id": "M7nyh-Zl5ma2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6\n",
        "\n",
        "Encoding"
      ],
      "metadata": {
        "id": "H2kDee-nveIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pop out the target variable\n",
        "y_target = df.pop('y')"
      ],
      "metadata": {
        "id": "w0GsxQ91vyt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode categorical columns in dataset\n",
        "df_encoded = pd.get_dummies(\n",
        "    df, columns=df.select_dtypes(include='object').columns.tolist())"
      ],
      "metadata": {
        "id": "JgIKjkWgyTGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nShape of the original DataFrame:\", df.shape)\n",
        "print(\"\\nShape of the encoded DataFrame:\", df_encoded.shape)\n",
        "\n",
        "print(\"Original DataFrame head:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nOne-Hot Encoded DataFrame head:\")\n",
        "display(df_encoded.head())\n",
        "\n",
        "print(\"\\nColumns of the encoded DataFrame:\")\n",
        "display(df_encoded.columns.tolist())"
      ],
      "metadata": {
        "id": "EihQ_t3fzwz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make a column by column explanation for one-hot encoding your predictor dataframe**\n",
        "\n",
        "note: numerical columns will be unaffected by dummy encoding. the following have been one-hot encoded:\n",
        "\n",
        "* **work class** - each work class will be expanded into it's own binary column.\n",
        "* **education** - each education level will be expanded into it's own binary column. When one-hot encoding, the education column's ordinality is lost, but the education-num column maintains the ordinality lost in encoding.\n",
        "* **marital-status** - each education level will be expanded into its own binary column.\n",
        "* **occupation** - each education level will be expanded into it's own binary column.\n",
        "* **relationship** - each relationship will be expanded into it's own bianry column.\n",
        "* **race** - each education level will be expanded into it's own binary column.\n",
        "* **sex** - each sex will be expanded into it's own binary column.\n",
        "* **native-country** - Each contry will be expanded into it's own binary column."
      ],
      "metadata": {
        "id": "Re1idIejy94-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe the difference pre and post encoding of the dataframe. How has the shape changed?**\n",
        "\n",
        "The dimensionality has increased tenfold. Specifically, there were 15 columns, and now there are 108, which will drastically increase the time it takes to train each model."
      ],
      "metadata": {
        "id": "GcSE08D8F43W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7\n",
        "\n",
        "Modeling"
      ],
      "metadata": {
        "id": "Mh9whP9T49Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a dummy model using a majority classifier\n",
        "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
        "dummy_clf.fit(df_encoded, y_target)"
      ],
      "metadata": {
        "id": "SL1jS9m_5L5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1 - Decision Tree - 75 models - 2 min execution\n",
        "tree_params = {\n",
        "    'ccp_alpha': [0.0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 2, 5, 10, 20]\n",
        "}\n",
        "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "tree_clf = tree.DecisionTreeClassifier()\n",
        "tree_grid = GridSearchCV(tree_clf, tree_params, cv=5, scoring=scoring, refit='accuracy')\n",
        "tree_grid.fit(df_encoded, y_target)"
      ],
      "metadata": {
        "id": "EAcwLjcWhbS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 4 - K Nearest Neighbors (KNN) - 42 models -\n",
        "knn_params = {\n",
        "    'n_neighbors': [i for i in range(1, 50, 5)],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_grid = GridSearchCV(knn_clf, knn_params, cv=3, scoring=scoring, refit='accuracy')\n",
        "knn_grid.fit(df_encoded, y_target)"
      ],
      "metadata": {
        "id": "5RDUaG67h5Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2 - Support Vector Machine (SVM) - 55 models -\n",
        "\n",
        "svc_params = {\n",
        "    'C': [0.1, 1, 5, 10, 30, 50, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "}\n",
        "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "svc_clf = SVC(random_state=42)\n",
        "svc_grid = GridSearchCV(svc_clf, svc_params, cv=5, scoring=scoring, refit='accuracy')\n",
        "svc_grid.fit(df_encoded, y_target)"
      ],
      "metadata": {
        "id": "ASZD3WKmhd4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3 - Multilayer Perceptron (MLP) Nueral Network\n",
        "\n"
      ],
      "metadata": {
        "id": "gFa8i38ihuJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8\n",
        "\n",
        "Model Comparison"
      ],
      "metadata": {
        "id": "yrUQg2qJiSME"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P58tBwZOijgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EUydvjBWihyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9"
      ],
      "metadata": {
        "id": "i4uQBZ0EifvG"
      }
    }
  ]
}